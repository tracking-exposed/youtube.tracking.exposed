---
title: "1st Global coordinated observation of the Youtube algorithm"
subtitle: "Only by compare how AI mistreat you and others, we can grasp how this is a collective issue"
draft: false
date: 2020-03-01T10:26:08Z

og_title: "1st Global collaborative analysis of Youtube Algorithm"
og_type: "website"
og_image: "http://youtube.tracking.exposed/images/compare.jpeg"
og_url: "https://youtube.tracking.exposed/wetest/1"
og_description: "This is the first worldwide test of the Youtube algorithm; on Sunday March 15th, with a browser extension, we'll see how YT personalizes the customer experience"

extraCSS: "/css/wetest.css"
---

<script src="/js/collaborative-tests.js"></script>

<div class="container col-12 justify-content-center">
  <h1 style="text-align:center;">Experiment — 15 of March 2020 — in
    <span class="project-color"> <span id="demo"></span> </span>
   </h1>
</div>

<div class="container row">
  <img width="48%" class="align-right imgtile" src="/images/wetest-youtrust.jpg" />
  <img width="48%" class="align-left imgtile" src="/images/wetest-how.jpg" />
</div>

<br>
{{<colorblock text="a 10 minutes experiment where every contribution matters.">}}
<br>

<div class="container col-12 justify-content-center">
  <h2 class="project-color">FIRST ― To save evidence of personalization join us: </h2>
  {{<yt-extension>}}
</div>

{{<colorblock >}}
<br>

<div class="container col-12 justify-content-center">
  <h2 class="project-color">SECOND ― Following the links, the order matter!</h2>

  <small>The links will be declared few hours before the test start!</small>
  <div class="test-steps links--disabled">
    <ol>
      <li>
        Open <a href="https://www.youtube.com" target="_blank">YouTube Homepage</a>.
      </li>
      <li>
        Watch The first video: <a href="https://www.youtube.com/watch?v=" target="_blank">Which is still to be defined!</a>!.
      </li>
      <li>
        Watch The second video: <a href="https://www.youtube.com/watch?v=" target="_blank">Which is still to be defined!</a>!.
      </li>
      <li>
        Watch The third video: <a href="https://www.youtube.com/watch?v=" target="_blank">Which is still to be defined!</a>!.
      </li>
      <li>
        Watch The fourth video: <a href="https://www.youtube.com/watch?v=" target="_blank">Which is still to be defined!</a>!.
      </li>
      <li>
        Open againg <a href="https://www.youtube.com" target="_blank">YouTube Homepage </a> 
      </li>
    </ol>
    <small>Why have we <a href="/wetest/announcement-1#on-experiment-design">organized the test this way</a>?</small>
    <br>
    <br>
  </div>

</div> <!-- container -->

{{<colorblock >}}

<div class="container col-12 justify-content-center">
  <br>
  <h2 class="project-color">THIRD ― Compare, analyze, understand</h2>
  <br>

  <div class="row enlarged">
    <div class="col-4">
      We'll release the data, properly anonymized, for public analysis, 
      <a href="/wetest/announcement-1">Here you'll find releases and updates</a>.
    </div>
    <div class="col-4">
      If you produce findings or visualization, we'll be glad to include yours, so please <a href="https://chat.securitywithoutborders.org/community/channels/trackingexposed">reach out in our mattermost chat</a>.
    </div>
    <div class="col-4">
      We'll share soon an expected timeline of releases and we'll keep
        <a href="/wetest/announcement-1">updating this page</a>.
    </div>
  </div> 

</div> <!-- container -->

<br>
{{<colorblock >}}

<div class="container col-12 justify-content-center">
  <br>
  <h2 class="project-color">EXTRA ― Know more, on us, on this test design and what should happen next</h2>
  <br>
  <div class="row enlarged">
    <div class="col-6">
      <h3>weTEST Experiment Design</h3>
      <p>Testing a personalization algorithm isn't quick and straightforward, as it can seem. The researcher has to define a methodology. This method gives different values to the collected samples because they are only useful to test the assumptions initially made. 
        <br>
        <br>

        <i>This text explore a difficult concept. It might take a while before fully interiorize the complexity of tool, restrictions, and variables tracking. If anything is unclear, don't miss the
          <a href="/automation"><b>
            three different methodologies lead to different dataset properties
          </b></a>,
          <a href="/what-we-collect"><b>
            what we collect and how data might be safely released
          </b> </a>, and 
          <a href="/data"><b>
            data format and usages
          </b></a>.
        </i>
        <br>
        <br>
        Because wetest wants to be a collaborative experiment, it is smart to develop initial findings without assuming any past research. We have them, but we'll start with a few concise research questions. 
        <br>
        Inevitability these question would be closer to the technical side, and not to the political impact. The initial phase we face is perfect for developing tools and practices. 
        <br>
        <br>
        Tracking.exposed goals aren't merely to produce reports, articles, or researches. Yes, we do it, (look at our <a href="/">home</a>, bottom-left); it has been part of our training experience. Algorithm accountability can't be revolutionary if accessible only to data analyst and data protection authority. A bit of knowledge on platform influence, or algorithm literacy, should be in the modern background education, and we want to play with it. 
        <br>
        <br>
        Do you remember, companies taking a bunch of random users and experiment on them? 
          <b>We'll do precisely the opposite</b> (or, opposition?): 
          <b>a distributed crowd of random individuals, coordinated to experiment on them!</b>
          Anyone can contribute. Someone with a clean, freshly installed browser, someone else with their Google account logged. Partecipants should watch a few videos, and because we're concentrating the check on four selected contents, in one day, we will run comparison to see how much diverse ends up to be suggested next to a video. 
          <br<br>
          It is the most basic calculus on how much is unique, personalized, the experience the company decide for us.
      </p>

    </div>
    <div class="col-6">
      <img class="imgtile" width="96%" style="margin-left:2%;margin-right:2%" src="/images/wetest-youralgo.jpg" />
      <br>
      <smaller style="font-size:0.6em;">This is our 
        <a href="https://tracking.exposed/manifesto">Manifesto </a>, or checkout the 
        <a href="https://pornhub.tracking.exposed/potest/final-1">collaborative PornHub analysis</a>, and
        <a href="/preview">use ytTREX</a>.
      </smaller>
      <br>
      <h3>This test - why language?</h3>
      <blockquote class="blockquote">
        <p class="mb-0">
          Farshad Shadloo, a YouTube spokesman, said the company’s recommendations aimed to steer people toward authoritative videos that leave them satisfied. He said the company was continually improving the algorithm that generates the recommendations. “Over the past year alone, we’ve launched over 30 different changes to reduce recommendations of borderline content and harmful misinformation, including climate change misinformation and other types of conspiracy videos,” he said. “Thanks to this change, watchtime this type of content gets from recommendations has dropped by over 70 percent in the U.S.”
        </p>
        <footer class="blockquote-footer" style="margin-top:0;border-top:0;">NYTimes 2 March 2020
          <cite title="Source Title">Can YouTube Quiet Its Conspiracy Theorists?</cite>
        </footer>
      </blockquote>
      <p>
        In commentary such as the one above, there are two pitfalls: <b>a language issue</b>, and a <b>trust issue</b>.
        <br>
        Maybe YT is right, and in English language, for the US audience, Youtube investment might guarantee a quality in content curaction high enough to avoid fines, but would be that true in another language? 
        <br>
        The exploitative business model of surveillance capitalism can't easily scale when the succes metric is the _removal of content troublesome for a precise culture_ because investing in content moderator trained and balanced in every culture in the world seems unfeasable due to high costs.
        <br>
        This first experiment concentrate effort in watching four videos chosen by us, on the same broad topic (covid19), in the four most spread langagues in the world.
        <br>
        Regardless of trust, how is credible a company who promise changes, (they are invisible), and the quality assessment of such improvements comes from the same organization? Independent testing, like the one we want to enable, is an option accessible to you, to a class of students, to the FTC and to the DPAs.
      </p>
    </div>
  </div> 

  <h3>
    We want to apply the most scientific, open, distributed approach we can aim for
  </h3>
  <p>
    <b>
      Diversity is the key
    </b>, but how exactly?
    <br>
    <br>
    Personalization algorithms, content curation, and targeted experiences are unique for each of us. 
    <br>
    Winning the fight for algorithmic independence (when you retain agency and control on the prioritization filter know as recommendation system), make sense if most of us reach that point. A minority of techno elite, literate enough to fact-check and has a bunch of tricks to find the right information, would only reinforce inequity in the information age. Diversity is the key because we collectively should understand how other people are perceiving the public discourse. 
    <br>
    Recommendation algorithm is public policy, they should be subject to public scrutiny, impact all of us, and companies trim them to regulate fringe behaviors. Just in 2020, the goal is not a better society but a fluid business flow in the monopolist's hand.
    <br>
    Algorithm analysis might be a purely technical effort in a fully known system. But the platform we're operating on (Facebook, youtube, amazon) mixes social constructs with their technology, and implicitly, the limited form of algorithm analysis we can perform via passive observation, inherit complexities typical of political analysis.
    <br>
    <br>
    Now, with wetest#1, <b>we begin with the technical analysis</b>, to build up a knowledge base robust enough to address <b>political and sociological analysis</b>. We can't yet address research questions such as "<i>Do youtube radicalize or not people?</i>" or "<i>are videos with blonde white women prioritized against other demographics? It is true in any region?</i>". We want to coordinate tests with these politically meaningful topics, but we can't yet, they aren't low hanging fruits. Releasing approximative analysis would be detrimental for algorithm literacy and platform accountability. Let's build this community with academics and digital rights defender. 
    <br>
    If you have an idea, propose your experiment by
    <a href="https://github.com/tracking-exposed/youtube.tracking.exposed/issues/new?assignees=&labels=research+question&template=research-question-proposal.md&title=%3CRQ%3E" target=_blank>opening this formatted GitHub issue</a>
    , and please consider:
    <ol>
      <li>we should start to measure technical conditions, and be confident in testing such variables.</li>
      <li>read other issues marked in the same way; this might help to understand which limits this test has.</li>
      <li>research questions should come from the community: concerned citizen, seasoned professional expert, the method to submit is to open a GitHub issue in our repository, and if you expect</li>
    </ol>
  </p>

</div> <!-- container -->

<script>
  $(document).ready(function() {
    countdown(new Date("Mar 15, 2020 00:00:01"), "demo");
  });
</script>